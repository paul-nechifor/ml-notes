\documentclass[12pt]{article}

% A5 paper with tiny margins. Works well on a tablet.
\usepackage[a5paper, top=0.2cm, right=0.2cm, bottom=0.2cm, left=0.2cm]{geometry}
\usepackage[english]{babel}
\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{multirow}

\newcommand*\sepstars{%
  \begin{center}
    $\star\star\star$
  \end{center}
}

\newcommand{\hitem}[1][default] {
    \item[#1] \hfill \\
}

\newcommand{\litem}[2][default] {
    \item[#1] \hfill
    \begin{itemize}
        {#2}
    \end{itemize}     
}

\title{Machine Learning: Personal Notes}
\author{Paul Nechifor}

\begin{document}

\maketitle

\section{Machine Learning}

\subsection{Decision Trees}

\begin{description}

\item[entropy] (im)purity of an arbitrary collection of examples

\item[entropy of a collection]

\item[boolean classification]

\hitem[entropy]
Minimum number of bits needed to encode the classification of an arbitrary
member of the collection (drawn at random with uniform probability). For
boolean, if the number of positives is 1, the entropy is 0 since no information
needs to be communicated. If it is 0.5, one bit is required. If it's 0.8 (or
0.2), less than one bit is needed (on average).

For a boolean classification:

\begin{equation}
    Entropy(S) \equiv -p_\oplus \log_2 p_\oplus - p_\ominus \log_2 p_\ominus
    \notag
\end{equation}

General form:

\begin{equation}
    Entropy(S) \equiv \sum\limits_{i=1}^c -p_i \log_2 p_i
    \notag
\end{equation}

\item[information gain] Measures the expected reduction in entropy.

\end{description}


\section{Statistical Learning}

\subsection{Chapter 1}

\begin{description}

    \item[learning] supervised/unsupervised
    
    \litem[supervised learning starting point]{
        \item vector of $p$ predictor measurements $X$ (input)
        \item outcome measurement $Y$ (response)
        \item in \emph{regression}, $Y$ is quantitative (e.g. price)
        \item in \emph{classification}, $Y$ takes values from a set (e.g.
            survived/died)
    }
    
    \item[unsupervised learning] objective is more fuzzy
    
    \litem[machine vs statistical learning]{
        \item ML emphasizes large scale applications and prediction accuracy
        \item SL emphasizes models, their interpretability, precision and
            and uncertainty        
    }
    
\end{description}

\subsection{Chapter 2}

\begin{description}

    \litem[notation]{
        \item $X_1$ is called a feature, input, or predictor
        \item $x$ is the input vector
        \item we write our model as $Y = f(X) + \epsilon$, where $\epsilon$
        measures the errors
        \item hats (~$\hat{}$~) indicate estimation from training data
    }
    
    \litem[purpose of $f(X)$]{
        \item make predictions of $Y$ at new points
        \item understand which components of $X$ are important and how each of
        	them affects $Y$
    }
    
    \litem[regression function]{
    	\item $f(x) = E(Y|X=x)$ (expected value of $Y$ given $X = x$)
    	\item since there are few or no points for every precise $x$ we relax
    		the definition:
    		\begin{equation}
    			\hat{f}(x) = Ave(Y|X \in N(x)) \notag
    		\end{equation}
    		where $N(x)$ is some neighborhood of $x$
    	\item nearest neighbor is good for small $p$ (e.g. $p \le 4$) and large
    		$N$
	}
	
	\item[the curse of dimensionality] nearest neighbors tend to be far away in
		high dimensions
		
	\litem[parametric models, structured models]{
		\item linear model (type of parametric model) (specified in terms of
			$p+1$ parameters
		\begin{equation}
			f_L(X) = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p \notag
		\end{equation}
		
		\item quadratic model
	}

    \litem[book notation]{
        \item $n$ = number of data points, $p$ = number of variables
    }

    \item[variance] Var($\epsilon$) (it's irreducible)

    \item[reducible, irreducible]

    \item[overfitting]

    \item[mean squared error] $MSE = \frac{1}{n}\sum\limits_{i=1}^n (y_i -
    	\hat{f}(x_i))^2$

    \item[assessing model accuracy] fit a model $\hat{f}(x)$ to some training
    	data Tr, but this will be biased toward overfit models, so test on
    	a fresh test data Te

    \item[bias-variance trade-off]
    
    \item[Bayes optimal classifier]
    
\end{description}

\end{document}
